{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3 - RFM\n",
    "\n",
    "## Author - Lukas Fiorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project uses data from Tableau Public on Customer transactions to compute RFM (Recency, Frequency, Monetary Value) and then segment customer orders through kmeans clustering.\n",
    "The data can be found and downloaded here: \n",
    "- https://community.tableau.com/s/contentdocument/0694T000001GnpUQAS\n",
    "\n",
    "For this project, we download the data from a link made available through University of Washington.\n",
    "- https://library.startlearninglabs.uw.edu/DATASCI420/2019/Datasets/SuperstoreTransaction.csv\n",
    "\n",
    "#### Abstract\n",
    "\n",
    "Our data consists of \\~10,000 products bought in ~5,000 orders made by ~800 customers from 2014-2017.  Our goal is to engineer new features to inform customer segmentation for promotional targeting.\n",
    "\n",
    "Specifically, we aggregate product purchases to customer orders and compute RFM, then (kmeans) cluster those orders based on our new quantitative features measuring recency, frequency, and value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from IPython.display import display\n",
    "import textwrap\n",
    "\n",
    "import scipy.stats as ss\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set package options\n",
    "pd.set_option('display.max_columns', None) # show all columns\n",
    "plt.rc('patch', force_edgecolor = True) # set bar borders in bar plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data\n",
    "\n",
    "Our first step is to load the transaction data for analysis. \n",
    "\n",
    "The data is originally sourced from Tableau Public [(available here)](https://community.tableau.com/s/contentdocument/0694T000001GnpUQAS), although in this project we source directly through University of Washington."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate & load dataset\n",
    "file = \"https://library.startlearninglabs.uw.edu/DATASCI420/2019/Datasets/SuperstoreTransaction.csv\"\n",
    "ss_txn = pd.read_csv(file) # read file into df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading, we next preview the data. We see about 10,000 transactions and 21 attributes.  Visually, it appears that most of the attributes are not numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9994, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row ID</th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Ship Date</th>\n",
       "      <th>Ship Mode</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>Customer Name</th>\n",
       "      <th>Segment</th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Postal Code</th>\n",
       "      <th>Region</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sub-Category</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>CA-2016-152156</td>\n",
       "      <td>11/8/2016</td>\n",
       "      <td>11/11/2016</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>CG-12520</td>\n",
       "      <td>Claire Gute</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Henderson</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>42420</td>\n",
       "      <td>South</td>\n",
       "      <td>FUR-BO-10001798</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Bookcases</td>\n",
       "      <td>Bush Somerset Collection Bookcase</td>\n",
       "      <td>261.9600</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41.9136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>CA-2016-152156</td>\n",
       "      <td>11/8/2016</td>\n",
       "      <td>11/11/2016</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>CG-12520</td>\n",
       "      <td>Claire Gute</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Henderson</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>42420</td>\n",
       "      <td>South</td>\n",
       "      <td>FUR-CH-10000454</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Chairs</td>\n",
       "      <td>Hon Deluxe Fabric Upholstered Stacking Chairs,...</td>\n",
       "      <td>731.9400</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>219.5820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>CA-2016-138688</td>\n",
       "      <td>6/12/2016</td>\n",
       "      <td>6/16/2016</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>DV-13045</td>\n",
       "      <td>Darrin Van Huff</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>United States</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>California</td>\n",
       "      <td>90036</td>\n",
       "      <td>West</td>\n",
       "      <td>OFF-LA-10000240</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Labels</td>\n",
       "      <td>Self-Adhesive Address Labels for Typewriters b...</td>\n",
       "      <td>14.6200</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.8714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>US-2015-108966</td>\n",
       "      <td>10/11/2015</td>\n",
       "      <td>10/18/2015</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>SO-20335</td>\n",
       "      <td>Sean O'Donnell</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Fort Lauderdale</td>\n",
       "      <td>Florida</td>\n",
       "      <td>33311</td>\n",
       "      <td>South</td>\n",
       "      <td>FUR-TA-10000577</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Tables</td>\n",
       "      <td>Bretford CR4500 Series Slim Rectangular Table</td>\n",
       "      <td>957.5775</td>\n",
       "      <td>5</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-383.0310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>US-2015-108966</td>\n",
       "      <td>10/11/2015</td>\n",
       "      <td>10/18/2015</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>SO-20335</td>\n",
       "      <td>Sean O'Donnell</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Fort Lauderdale</td>\n",
       "      <td>Florida</td>\n",
       "      <td>33311</td>\n",
       "      <td>South</td>\n",
       "      <td>OFF-ST-10000760</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Storage</td>\n",
       "      <td>Eldon Fold 'N Roll Cart System</td>\n",
       "      <td>22.3680</td>\n",
       "      <td>2</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2.5164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Row ID        Order ID  Order Date   Ship Date       Ship Mode Customer ID  \\\n",
       "0       1  CA-2016-152156   11/8/2016  11/11/2016    Second Class    CG-12520   \n",
       "1       2  CA-2016-152156   11/8/2016  11/11/2016    Second Class    CG-12520   \n",
       "2       3  CA-2016-138688   6/12/2016   6/16/2016    Second Class    DV-13045   \n",
       "3       4  US-2015-108966  10/11/2015  10/18/2015  Standard Class    SO-20335   \n",
       "4       5  US-2015-108966  10/11/2015  10/18/2015  Standard Class    SO-20335   \n",
       "\n",
       "     Customer Name    Segment        Country             City       State  \\\n",
       "0      Claire Gute   Consumer  United States        Henderson    Kentucky   \n",
       "1      Claire Gute   Consumer  United States        Henderson    Kentucky   \n",
       "2  Darrin Van Huff  Corporate  United States      Los Angeles  California   \n",
       "3   Sean O'Donnell   Consumer  United States  Fort Lauderdale     Florida   \n",
       "4   Sean O'Donnell   Consumer  United States  Fort Lauderdale     Florida   \n",
       "\n",
       "   Postal Code Region       Product ID         Category Sub-Category  \\\n",
       "0        42420  South  FUR-BO-10001798        Furniture    Bookcases   \n",
       "1        42420  South  FUR-CH-10000454        Furniture       Chairs   \n",
       "2        90036   West  OFF-LA-10000240  Office Supplies       Labels   \n",
       "3        33311  South  FUR-TA-10000577        Furniture       Tables   \n",
       "4        33311  South  OFF-ST-10000760  Office Supplies      Storage   \n",
       "\n",
       "                                        Product Name     Sales  Quantity  \\\n",
       "0                  Bush Somerset Collection Bookcase  261.9600         2   \n",
       "1  Hon Deluxe Fabric Upholstered Stacking Chairs,...  731.9400         3   \n",
       "2  Self-Adhesive Address Labels for Typewriters b...   14.6200         2   \n",
       "3      Bretford CR4500 Series Slim Rectangular Table  957.5775         5   \n",
       "4                     Eldon Fold 'N Roll Cart System   22.3680         2   \n",
       "\n",
       "   Discount    Profit  \n",
       "0      0.00   41.9136  \n",
       "1      0.00  219.5820  \n",
       "2      0.00    6.8714  \n",
       "3      0.45 -383.0310  \n",
       "4      0.20    2.5164  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ss_txn.shape)\n",
    "ss_txn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Structure\n",
    "\n",
    "Explicitly examining our data structure shows that our data is indeed mostly string data.\n",
    "\n",
    "Conveniently, we also see that there are no missing values in our data.\n",
    "\n",
    "*NB: although `Postal Code` is parsed as numeric, we would treat it as categorical for any analytical purposes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9994 entries, 0 to 9993\n",
      "Data columns (total 21 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Row ID         9994 non-null   int64  \n",
      " 1   Order ID       9994 non-null   object \n",
      " 2   Order Date     9994 non-null   object \n",
      " 3   Ship Date      9994 non-null   object \n",
      " 4   Ship Mode      9994 non-null   object \n",
      " 5   Customer ID    9994 non-null   object \n",
      " 6   Customer Name  9994 non-null   object \n",
      " 7   Segment        9994 non-null   object \n",
      " 8   Country        9994 non-null   object \n",
      " 9   City           9994 non-null   object \n",
      " 10  State          9994 non-null   object \n",
      " 11  Postal Code    9994 non-null   int64  \n",
      " 12  Region         9994 non-null   object \n",
      " 13  Product ID     9994 non-null   object \n",
      " 14  Category       9994 non-null   object \n",
      " 15  Sub-Category   9994 non-null   object \n",
      " 16  Product Name   9994 non-null   object \n",
      " 17  Sales          9994 non-null   float64\n",
      " 18  Quantity       9994 non-null   int64  \n",
      " 19  Discount       9994 non-null   float64\n",
      " 20  Profit         9994 non-null   float64\n",
      "dtypes: float64(3), int64(3), object(15)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "ss_txn.info() # data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Grain and Feature Cardinality ####\n",
    "\n",
    "Looking at unique values, we see that our data contains \\~10,000 product transactions (`Row ID`) bought across \\~5,000 orders (`Order ID`) made by \\~800 customers (`Customer ID`).\n",
    "\n",
    "Furthermore, we get a good sense of the cardinality of each column.\n",
    "\n",
    "Those with low cardinality (e.g. `Ship Mode`, `Segment`, `Region`, etc.) may be good candidates for onehot encoding.  Those with high cardinality would likely need to be sensibly aggregated first (to avoid increasing the dimensionality of our data too significantly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row ID           9994\n",
       "Order ID         5009\n",
       "Order Date       1237\n",
       "Ship Date        1334\n",
       "Ship Mode           4\n",
       "Customer ID       793\n",
       "Customer Name     793\n",
       "Segment             3\n",
       "Country             1\n",
       "City              531\n",
       "State              49\n",
       "Postal Code       631\n",
       "Region              4\n",
       "Product ID       1862\n",
       "Category            3\n",
       "Sub-Category       17\n",
       "Product Name     1850\n",
       "Sales            5825\n",
       "Quantity           14\n",
       "Discount           12\n",
       "Profit           7287\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss_txn.nunique() # count of unique values in each field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numeric Attributes\n",
    "\n",
    "Describing our numeric fields is really only useful for `Sales`, `Quantity`, `Discount`, and `Profit`.  We can't interpret much from the descriptive statistics of `Row ID` and `Postal Code`.\n",
    "\n",
    "We see that a typical order (median) is about \\\\$55, with profit just under \\$10.  Based on the max values, it looks like our data may skew right (as is typical for financial data).\n",
    "\n",
    "We apply a lambda function to round the stats for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row ID</th>\n",
       "      <th>Postal Code</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9994</td>\n",
       "      <td>9994</td>\n",
       "      <td>9994</td>\n",
       "      <td>9994</td>\n",
       "      <td>9994</td>\n",
       "      <td>9994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4997.5</td>\n",
       "      <td>55190.4</td>\n",
       "      <td>229.858</td>\n",
       "      <td>3.78957</td>\n",
       "      <td>0.156203</td>\n",
       "      <td>28.6569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2885.16</td>\n",
       "      <td>32063.7</td>\n",
       "      <td>623.245</td>\n",
       "      <td>2.22511</td>\n",
       "      <td>0.206452</td>\n",
       "      <td>234.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1</td>\n",
       "      <td>1040</td>\n",
       "      <td>0.444</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-6599.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2499.25</td>\n",
       "      <td>23223</td>\n",
       "      <td>17.28</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.72875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4997.5</td>\n",
       "      <td>56430.5</td>\n",
       "      <td>54.49</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>8.6665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7495.75</td>\n",
       "      <td>90008</td>\n",
       "      <td>209.94</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>29.364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9994</td>\n",
       "      <td>99301</td>\n",
       "      <td>22638.5</td>\n",
       "      <td>14</td>\n",
       "      <td>0.8</td>\n",
       "      <td>8399.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Row ID Postal Code    Sales Quantity  Discount    Profit\n",
       "count     9994        9994     9994     9994      9994      9994\n",
       "mean    4997.5     55190.4  229.858  3.78957  0.156203   28.6569\n",
       "std    2885.16     32063.7  623.245  2.22511  0.206452    234.26\n",
       "min          1        1040    0.444        1         0  -6599.98\n",
       "25%    2499.25       23223    17.28        2         0   1.72875\n",
       "50%     4997.5     56430.5    54.49        3       0.2    8.6665\n",
       "75%    7495.75       90008   209.94        5       0.2    29.364\n",
       "max       9994       99301  22638.5       14       0.8   8399.98"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss_txn.describe().apply(lambda s: s.apply(lambda x: format(x, 'g'))) # data stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep\n",
    "\n",
    "Next we'll clean up our column names and convert our date columns from objects to datetime fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns for consistency\n",
    "ss_txn.columns = ss_txn.columns.str.replace(\"-\", \"_\") # replace hyphens\n",
    "ss_txn.columns = ss_txn.columns.str.replace(\" \", \"_\") # replace white space\n",
    "ss_txn.columns = ss_txn.columns.str.lower() # lower case\n",
    "\n",
    "# convert date columns to datetime (from object)\n",
    "ss_txn['order_date'] = pd.to_datetime(ss_txn['order_date'])\n",
    "ss_txn['ship_date'] = pd.to_datetime(ss_txn['ship_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization\n",
    "\n",
    "In this section we'll explore our dataset a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll store the count of observations (transactions) by `order_date`.  From the preview, it looks like our transaction data spans from 2014 - 2017 (`.sort_index()` is used to ensure the date valus are sorted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2014-01-03     1\n",
       "2014-01-04     3\n",
       "2014-01-05     1\n",
       "2014-01-06     9\n",
       "2014-01-07     2\n",
       "              ..\n",
       "2017-12-26     4\n",
       "2017-12-27     2\n",
       "2017-12-28    19\n",
       "2017-12-29    12\n",
       "2017-12-30     7\n",
       "Name: order_date, Length: 1237, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txn_count_daily = ss_txn['order_date'].value_counts().sort_index() # sort values\n",
    "txn_count_daily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transaction Volume Trends\n",
    "\n",
    "Plotting transaction counts by day, we get confirmation that our data spans from January 2014 thru December 2017 (4 years of transactions).  We also see that the data is pretty noisy (fairly volatile from day-to-day).\n",
    "\n",
    "Applying a 14-day rolling average helps make the overall upward trend a bit more visible.  We also see some seasonal spikes in the latter parts of each year (September, November, December) before dropping back down in January."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set figure, axis properties\n",
    "plt.figure(figsize = (16, 6))\n",
    "plt.xlabel(\"Transaction Date\")\n",
    "plt.ylabel(\"Transaction Count\")\n",
    "plt.title(\"Daily Transaction Volume\")\n",
    "\n",
    "# draw line plots\n",
    "sns.lineplot(x = txn_count_daily.index, y = txn_count_daily, alpha = 0.25, label = 'daily');\n",
    "sns.lineplot(x = txn_count_daily.index, y = txn_count_daily.rolling(14).mean(), label = '14d avg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: Feature Distributions\n",
    "\n",
    "For a bit more exploration, we also define a function to show distributions of categorical and continuous variables.  Setting `plot_type` to `'distplot'` works best for continuous features, while `'countplot'` is more natural for categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_plots(df, col_list, plot_type, n_cols=2):\n",
    "    '''\n",
    "    df: dataframe\n",
    "    col_list: list of column names.\n",
    "    plot_type: ['countplot', 'distplot']\n",
    "    n_cols: number of columns in grid of plots. default = 2.\n",
    "    '''\n",
    "    \n",
    "    # deduce nbr of rows needed for plot\n",
    "    n_rows = math.ceil(len(col_list) / n_cols)\n",
    "\n",
    "    # set figure, axis properties\n",
    "    fig, ax = plt.subplots(ncols = n_cols, nrows = n_rows)\n",
    "    fig.set_size_inches(16, 4 * n_rows)\n",
    "    fig.tight_layout(h_pad = 6, w_pad = 12)\n",
    "    \n",
    "    params = {'kde': False} if plot_type == 'distplot' else {}\n",
    "\n",
    "    # if one column only, plot just that one\n",
    "    if len(col_list) == 1:\n",
    "        sns.countplot(y = df[col_list[0]], ax = ax);\n",
    "        ax.title.set_text('# of transactions, by ' + col_list[0]);\n",
    "    \n",
    "    # if multiple columns, loop & plot each\n",
    "    else:\n",
    "        # loop through each column & plot\n",
    "        for i, col in enumerate(col_list):\n",
    "\n",
    "            # compute row, col index\n",
    "            i_row = math.floor(i / n_cols)\n",
    "            i_col = i % n_cols\n",
    "\n",
    "            # make plots\n",
    "            getattr(sns, plot_type)(df[col], ax = ax[i_row][i_col], **params)\n",
    "            ax[i_row][i_col].title.set_text('Transactions, by ' + col);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Features (low-cardinality)\n",
    "\n",
    "Next, we identify categorical columns (those of type `'object'`), and then specifically our low-cardinality categorical columns for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store object columns\n",
    "obj_cols = ss_txn.select_dtypes(['object']).columns.tolist()\n",
    "\n",
    "# count of unique values by column\n",
    "ss_txn_nunique_obj = ss_txn[obj_cols].nunique().tolist()\n",
    "\n",
    "# store object columns with cardinality of 2-10\n",
    "obj_low_card = ss_txn[obj_cols].columns[\n",
    "    np.where(\n",
    "        (np.array(ss_txn_nunique_obj) >= 2) & \n",
    "        (np.array(ss_txn_nunique_obj) <= 10)\n",
    "    )[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transaction Counts by Category\n",
    "\n",
    "Transaction counts of categorical columns show that consumers are making the most purchases and office supplies are the most commonly purchased product types.\n",
    "\n",
    "Slower shipping options are more common and region is relatively balanced (although a bit fewer purchases from customers in the South).\n",
    "\n",
    "Does customer segment influence these other attributes at all? Are consumers more likely to buy office supplies than corporate?\n",
    "\n",
    "We'll take a look at that next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_plots(ss_txn, obj_low_card, plot_type = 'countplot', n_cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: Crosstab heatmaps\n",
    "\n",
    "Let's define a function to make those comparisons by segment for us. To call the function, we provide a data frame, a list of columns, and the column (`segment`, in this case) to split by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctab_heatmap(df, col_list, remove_col):\n",
    "\n",
    "    # remove specified column from low-cardinality list (for crosstabs)\n",
    "    col_list_ctab = col_list.tolist() # convert to list if not already\n",
    "    col_list_ctab.remove(remove_col)\n",
    "\n",
    "    # set figure properties\n",
    "    fig, ax = plt.subplots(ncols = 3, nrows = 1)\n",
    "    fig.set_size_inches(14, 8)\n",
    "    fig.tight_layout(w_pad = 12)\n",
    "\n",
    "    for i, col in enumerate(col_list_ctab):\n",
    "\n",
    "        # calculate crosstabs\n",
    "        ctab_txn_cnt = pd.crosstab(df[col], df[remove_col], margins = True, normalize = 'index')\n",
    "        cbar_fmt = lambda x, pos: '{:.0%}'.format(x)\n",
    "\n",
    "        # make plots\n",
    "        sns.heatmap(ctab_txn_cnt, \n",
    "                    cmap = 'Blues', \n",
    "                    square = True, \n",
    "                    cbar_kws = {'shrink': 0.3, 'format': FuncFormatter(cbar_fmt)}, \n",
    "                    annot = True, \n",
    "                    fmt = '.0%',\n",
    "                    linewidths = 1.0,\n",
    "                    ax = ax[i]\n",
    "                   );\n",
    "        \n",
    "        # set labels\n",
    "        ax[i].title.set_text('Product Transaction Mix by segment\\n Split by ' + col);\n",
    "        ax[i].set_yticklabels(ax[i].get_yticklabels(), rotation = 0, fontsize = 12)\n",
    "        ax[i].set_yticklabels(ax[i].get_yticklabels(), rotation = 0, fontsize = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transaction mix by segment\n",
    "\n",
    "We see that there's not much difference in Product transaction mix between customer segments.  Overall, about 52% are made by consumers, 30% by corporate, and 18% by home office.\n",
    "\n",
    "The only notable deviation that we see from that is on delivery type, where corporate is much less likely to use same day delivery than we'd otherwise expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctab_heatmap(ss_txn, obj_low_card, 'segment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numeric Features\n",
    "\n",
    "And now let's also identify numeric columns to visualize those distributions.\n",
    "\n",
    "The `row_id` and `postal_code` columns are both numeric, but we'll exclude them from our plots since those visualizations won't be helpful for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store numeric columns; excluding row_id, postal_code\n",
    "num_cols = ss_txn.select_dtypes(['float', 'int']).columns.tolist()\n",
    "num_cols = list(set(num_cols) ^ set(['row_id', 'postal_code']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transaction Distribution by Numeric Features\n",
    "\n",
    "Plotting numeric columns, we're able to learn a bit more about each.\n",
    "\n",
    "The `discount` field appears to be a proportion between 0 and 1.  From the plot, the relationship between `sales` and `discount` isn't clear, but we can examine that next.  The primary question is whether the `sales` data is inclusive of the discount.\n",
    "\n",
    "`profit` appears to be the closest to normally distributed and is centered just above 0 (most product transactions are profitable, although some are not).\n",
    "\n",
    "`sales` and `quantity` both have long right tails.  `sales` in particular may be a candidate for log-transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_plots(ss_txn, num_cols, plot_type = 'distplot', n_cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `discount` inclusion\n",
    "\n",
    "Let's check whether the sales amount includes the discount or whether the discount still needs to be taken off.\n",
    "\n",
    "We need to control for the number of products in each order, so we'll first calculate the per-unit price (as `product_price`).\n",
    "\n",
    "Then we'll group by `product_id` and `discount` to see how product price varies based on the discount field.  Lastly, we'll also add a counter field for `product_id`, to make it a bit simpler to take subsets of product-level data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per unit price\n",
    "ss_txn['product_price'] = ss_txn['sales'] / ss_txn['quantity']\n",
    "\n",
    "# columns to group by & aggregate on\n",
    "group_cols = ['product_id', 'product_name', 'discount']\n",
    "agg_cols = ['product_price']\n",
    "\n",
    "# aggregate by product, discount\n",
    "product_discounts = ss_txn[group_cols + agg_cols].groupby(group_cols, as_index = False).mean()\n",
    "\n",
    "# compute product_num (product_id counter field)\n",
    "product_discounts['product_num'] = (product_discounts['product_id'].\n",
    "                                    transform(lambda x: pd.CategoricalIndex(x).codes)) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discount <ins>is</ins> included the listed sales amounts\n",
    "\n",
    "Now we can compare the product price on various products based on the discount field.  We'll plot product price for just the first 8 products in our dataset and inspect.\n",
    "\n",
    "From the plots, we see that product price is **already** inclusive of the discount % off.  Going up one level, this means that the `sales` field is also already inclusive of the awarded `discount`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make plot\n",
    "ax = sns.catplot(data = product_discounts[product_discounts['product_num'] <= 8], \n",
    "            x = 'discount', y = 'product_price', \n",
    "            col = 'product_name', col_wrap = 4,\n",
    "            kind = 'bar', color = 'blue', saturation = 0.35, aspect = 1.5);\n",
    "\n",
    "# set facet grid height padding\n",
    "ax.fig.tight_layout(h_pad = 10)\n",
    "\n",
    "# set plot title\n",
    "ax.fig.suptitle(\n",
    "    'Product Price by Discount Amount\\nSplit by Product', \n",
    "    fontsize = 40, \n",
    "    y = 1.25)\n",
    "\n",
    "# adjust font sizes\n",
    "for axes in ax.axes.flat:\n",
    "    axes.set_xticklabels(axes.get_xticklabels(), size = 24)\n",
    "    axes.set_yticklabels(axes.get_yticks(), size = 24)\n",
    "    axes.set_title(\n",
    "        textwrap.fill(axes.get_title(loc = 'center'), 30), # wrap product name after 30 chars\n",
    "        size = 24);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: Compare Distribution Fits of Numeric Attributes\n",
    "\n",
    "Now let's follow up on our numeric fields that were right skewed.\n",
    "\n",
    "From our earlier inspection, we saw that the `product_price` and `sales` features in particular had long right tails.  Next we define a function to apply transformation functions (log and boxcox), then compare the original & transformed distributions to the normal distribution.\n",
    "\n",
    "We'll make comparisons both statistically (using a kolmogorov-smirnov test) and visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to graphically and statistically compare distribution transformations\n",
    "def compare_transformations(df, col, const=0):\n",
    "    \n",
    "    # set plot and axis properties\n",
    "    fig, axs = plt.subplots(ncols = 3, nrows = 2)\n",
    "    fig.set_size_inches(12, 8)\n",
    "    fig.tight_layout(pad=6.0)\n",
    "    axs[0][0].title.set_text('QQ plot: ' + col)\n",
    "    axs[0][1].title.set_text('QQ plot: log(' + col + ')')\n",
    "    axs[0][2].title.set_text('QQ plot: boxcox(' + col + ')')\n",
    "    axs[1][0].title.set_text('distribution: ' + col)\n",
    "    axs[1][1].title.set_text('distribution: log(' + col + ')')\n",
    "    axs[1][2].title.set_text('distribution: boxcox(' + col + ')')\n",
    "\n",
    "    # store transformations\n",
    "    log_col = np.log(df[col] + const) # optionally specify a constant to avoid value=0 errors\n",
    "    # ignore np.NaN warnings specifically for boxcox transformation\n",
    "    with np.errstate(invalid='ignore'): \n",
    "        # optionally specify a constant to avoid value=0 errors\n",
    "        boxcox_col = pd.Series(ss.boxcox(df[col] + const)[0])\n",
    "    \n",
    "    # make QQ plots\n",
    "    sm.qqplot(df[col], line = 'q', ax = axs[0][0])\n",
    "    sm.qqplot(log_col, line = 'q', ax = axs[0][1])\n",
    "    sm.qqplot(boxcox_col, line = 'q', ax = axs[0][2])\n",
    "\n",
    "    # make distribution plots\n",
    "    sns.distplot(df[col], kde=False, ax = axs[1][0])\n",
    "    sns.distplot(log_col, kde=False, ax = axs[1][1])\n",
    "    sns.distplot(boxcox_col, kde=False, ax = axs[1][2]);\n",
    "    \n",
    "    # normalize columns for ks test\n",
    "    z_col = (df[col] - df[col].mean()) / df[col].std()\n",
    "    z_col_log = (log_col - log_col.mean()) / log_col.std()\n",
    "    z_col_boxcox = (boxcox_col - boxcox_col.mean()) / boxcox_col.std()\n",
    " \n",
    "    # print results of ks test\n",
    "    print(col + ':')\n",
    "    print(ss.kstest(z_col.dropna(), 'norm'))\n",
    "    \n",
    "    print('\\nlog(' + col + '):')\n",
    "    print(ss.kstest(z_col_log.dropna(), 'norm'))\n",
    "    \n",
    "    print('\\nboxcox(' + col + '):')\n",
    "    print(ss.kstest(z_col_boxcox.dropna(), 'norm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `product_price` distribution\n",
    "\n",
    "H<sub>0</sub>: The sample **is** drawn from a Normal distribution.\n",
    "\n",
    "Given the p-values, we reject that comparison.  Even after the log and box-cox transformations, it appears the tails are still too short to follow a normal distribution, although they are much closer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_transformations(ss_txn, 'product_price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sales` distribution\n",
    "\n",
    "H<sub>0</sub>: The sample **is** drawn from a Normal distribution.\n",
    "\n",
    "Similar to `product_price`, it appears that `sales` also does not follow a normal distribution even after the transformation (tails are still too short).\n",
    "\n",
    "That said - **in both cases, the log and boxcox transformations noticeably improved the right skew of the distributions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_transformations(ss_txn, 'sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate RFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate the data\n",
    "\n",
    "Now that we've gotten a sense for our dataset, we'll aggregate to the \"customer order day\" level.  Each record in our dataset will represent a unique customer and order date, rather than a unique customer product transaction.\n",
    "\n",
    "We'll start by grouping on `customer_id` and `order_date`, then summing up total sales (\\$ and #) and counting distinct orders and products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by customer id and order date\n",
    "# aggregate $ sales, # of total products, # of distinct products, # of distinct orders\n",
    "ss_txn_agg = ss_txn.groupby(['customer_id', 'order_date']).agg({\n",
    "    'sales': 'sum',\n",
    "    'quantity': 'sum', \n",
    "    'product_id': 'nunique',\n",
    "    'order_id': 'nunique'\n",
    "    })\n",
    "ss_txn_agg = ss_txn_agg.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previewing the data, we see several customer order dates and the corresponding sales attributes we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ss_txn_agg.head() # preview the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining our new structure, we see we've aggregated down to about half the row count that we began with (4,492 vs 9,994).\n",
    "\n",
    "We should note that this is <ins>not</ins> identical to grouping on customer orders, since a customer could make multiple orders on the same day.  In fact, we know from earlier in our exploration that there were 5,009 unique orders made, which is slightly more than the 4,492 records that we've aggregated to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_txn_agg.info() # data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, we can show exactly where the *customer order* level would not match the *customer order days* level.  Below we filter on just those records (where distinct count of `order_id` - for a given customer & day - is more than one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine records associated with an multi-order days\n",
    "ss_txn_agg[ss_txn_agg['order_id'] > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineer Recency, Frequency, Monetary metrics\n",
    "\n",
    "Next, we will use our aggregated data to calculate the recency, frequency, and value associated with each customer order day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take rolling sums at each check point, examining past 90 days\n",
    "roll_sum_sales = ss_txn_agg.groupby('customer_id').rolling(window = '90D', on = 'order_date')['sales'].sum()\n",
    "roll_cnt_products = ss_txn_agg.groupby('customer_id').rolling(window = '90D', on = 'order_date')['quantity'].sum()\n",
    "roll_cnt_products_distinct = ss_txn_agg.groupby('customer_id').rolling(window = '90D', on = 'order_date')['product_id'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute recency metric (days since customers last txn)\n",
    "roll_last_order = ss_txn_agg.groupby('customer_id')['order_date'].diff(periods = 1).dt.days\n",
    "\n",
    "# calculate max observed recency, for imputation\n",
    "last_order_imputation = (ss_txn_agg['order_date'].max() - ss_txn_agg['order_date'].min()).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine rolling sum and recency metrics\n",
    "ss_txn_roll = pd.concat([roll_sum_sales, roll_cnt_products, roll_cnt_products_distinct], axis = 1)\n",
    "ss_txn_roll.columns = ['dollar_roll_sum_90d', 'product_roll_sum_90d', 'distinct_product_roll_sum_90d']\n",
    "ss_txn_roll = ss_txn_roll.reset_index()\n",
    "\n",
    "# if first visit, fill w max date range\n",
    "ss_txn_roll['last_visit_ndays'] = roll_last_order.fillna(last_order_imputation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner merge agg & roll (on customer_id, order_date)\n",
    "ss_txn_rfm = ss_txn_agg.merge(\n",
    "    ss_txn_roll, \n",
    "    how = \"inner\", \n",
    "    on = ['customer_id', 'order_date'], \n",
    "    validate = 'one_to_one'\n",
    "    )\n",
    "\n",
    "# preview rfm data\n",
    "ss_txn_rfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_txn_rfm.info() # data structure of rfm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment using kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a copy of the dataset to handle numeric outliers/normalization for KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the dataset (no pointer)\n",
    "ss_txn_rfm_scaled = ss_txn_rfm.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the data\n",
    "\n",
    "- This is important for kmeans as it is a distance-based algorithm\n",
    "- select relevant numeric columns and z-scale them for use in kmeans\n",
    "- if data are not scaled, then distances will not be appropriately weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "scale_cols = ss_txn_rfm_scaled.columns.difference(['customer_id', 'order_date'], sort = False)\n",
    "ss_txn_rfm_scaled[scale_cols] = StandardScaler().fit_transform(ss_txn_rfm_scaled[scale_cols]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initialize & fit kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 2 # set number of clusters\n",
    "\n",
    "X = ss_txn_rfm_scaled[scale_cols]\n",
    "kmeans = KMeans(n_clusters = n_clusters, random_state = 0) # initialize kmeans\n",
    "kmeans.fit(X) # train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign cluster to each row of the original RFM data\n",
    "ss_txn_rfm['cluster'] = kmeans.predict(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_txn_rfm.head() # preview the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine and describe cluster centers\n",
    "\n",
    "- **cluster**: <ins>*Large, Frequent Transaction Days*</ins>\n",
    "  - The smaller cluster (23% of observations).  Defining characteristic: Larger, more frequent transaction days\n",
    "  - Transactions consist of more products and items\n",
    "    - However, the item::product ratio is similar to the other cluster, indicating that larger transaction size is primarily driven by the # of distinct products purchased (not a higher # of each product)\n",
    "  - Transactions are more frequent, although not overwhelmingly so.\n",
    "  - Larger rolling window spend, but mostly (not entirely) driven by the transaction itself.\n",
    "- **cluster**: <ins>*Small, Infrequent Transaction Days*</ins>\n",
    "  - The larger cluster (77% of observations).  Defining characteristic: Smaller, less frequent transaction days\n",
    "  - As opposed to the other cluster, these transactions tend to be smaller (due to fewer products purchased), and a bit less frequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine cluster centers\n",
    "pd.DataFrame(kmeans.cluster_centers_, columns=scale_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relative size of each cluster\n",
    "round(ss_txn_rfm['cluster'].value_counts(normalize = True).sort_index(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to examine means by cluster\n",
    "display(ss_txn_rfm.groupby(['cluster']).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
